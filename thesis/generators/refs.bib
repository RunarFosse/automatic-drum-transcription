@inproceedings{manilow2019cutting,
  title={Cutting Music Source Separation Some {Slakh}: A Dataset to Study the Impact of Training Data Quality and Quantity},
  author={Manilow, Ethan and Wichern, Gordon and Seetharaman, Prem and Le Roux, Jonathan},
  booktitle={Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year={2019},
  organization={IEEE}
}

@misc{callender2020improving,
    title={Improving Perceptual Quality of Drum Transcription with the Expanded Groove MIDI Dataset},
    author={Lee Callender and Curtis Hawthorne and Jesse Engel},
    year={2020},
    eprint={2004.00188},
    archivePrefix={arXiv},
    primaryClass={cs.SD}
}

@misc{vogl2018multiinstrumentdrumtranscription,
      title={Towards multi-instrument drum transcription}, 
      author={Richard Vogl and Gerhard Widmer and Peter Knees},
      year={2018},
      eprint={1806.06676},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/1806.06676}, 
}

@article{8350302,
  author={Wu, Chih-Wei and Dittmar, Christian and Southall, Carl and Vogl, Richard and Widmer, Gerhard and Hockman, Jason and Müller, Meinard and Lerch, Alexander},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={A Review of Automatic Drum Transcription}, 
  year={2018},
  volume={26},
  number={9},
  pages={1457-1483},
  keywords={Instruments;Task analysis;Speech processing;Spectrogram;Transient analysis;Rhythm;Music information retrieval;automatic music transcription;automatic drum transcription;machine learning;matrix factorization;deep learning},
  doi={10.1109/TASLP.2018.2830113}
}


@Article{signals4040042,
AUTHOR = {Zehren, Mickaël and Alunno, Marco and Bientinesi, Paolo},
TITLE = {High-Quality and Reproducible Automatic Drum Transcription from Crowdsourced Data},
JOURNAL = {Signals},
VOLUME = {4},
YEAR = {2023},
NUMBER = {4},
PAGES = {768--787},
URL = {https://www.mdpi.com/2624-6120/4/4/42},
ISSN = {2624-6120},
ABSTRACT = {Within the broad problem known as automatic music transcription, we considered the specific task of automatic drum transcription (ADT). This is a complex task that has recently shown significant advances thanks to deep learning (DL) techniques. Most notably, massive amounts of labeled data obtained from crowds of annotators have made it possible to implement large-scale supervised learning architectures for ADT. In this study, we explored the untapped potential of these new datasets by addressing three key points: First, we reviewed recent trends in DL architectures and focused on two techniques, self-attention mechanisms and tatum-synchronous convolutions. Then, to mitigate the noise and bias that are inherent in crowdsourced data, we extended the training data with additional annotations. Finally, to quantify the potential of the data, we compared many training scenarios by combining up to six different datasets, including zero-shot evaluations. Our findings revealed that crowdsourced datasets outperform previously utilized datasets, and regardless of the DL architecture employed, they are sufficient in size and quality to train accurate models. By fully exploiting this data source, our models produced high-quality drum transcriptions, achieving state-of-the-art results. Thanks to this accuracy, our work can be more successfully used by musicians (e.g., to learn new musical pieces by reading, or to convert their performances to MIDI) and researchers in music information retrieval (e.g., to retrieve information from the notes instead of audio, such as the rhythm or structure of a piece).},
DOI = {10.3390/signals4040042}
}



