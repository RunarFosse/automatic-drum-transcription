\pagenumbering{roman}

\begin{abstract}

\noindent \acrfull{ADT} remains a challenging task within the field of \acrfull{MIR}, especially when drum sounds are mixed with melodic instruments, otherwise called \acrfull{DTM}. The current standard methodology for solving such tasks is through deep learning. This thesis investigates how different architectures and dataset composition affect \acrshort{ADT} performance. Two studies were conducted: one comparing the performances of five different neural network architectures over four standard \acrshort{ADT} datasets, and another evaluating how training models on different dataset combinations impacts their generalization ability both on- and \acrfull{OOD}. A novel dataset, SADTP, is introduced for this latter study is utilized for \acrshort{OOD} evaluation. The first study concluded convolutional recurrent architectures perform the best across datasets, however strictly recurrent architectures and the Vision Transformer show promising performance on larger datasets. The second study shows that training on large and diverse datasets from multiple sources could improve both on- and \acrfull{OOD} generalization. These findings offer insight into how architecural and dataset choices influence generalization for \acrshort{ADT}.

\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
	\todo{Write proper acknowledgements}
	Est suavitate gubergren referrentur an, ex mea dolor eloquentiam, novum ludus suscipit in nec. Ea mea essent prompta constituam, has ut novum prodesset vulputate. Ad noster electram pri, nec sint accusamus dissentias at. Est ad laoreet fierent invidunt, ut per assueverit conclusionemque. An electram efficiendi mea.
	
	\vspace{1cm}
	\hspace*{\fill}\texttt{Runar Fosse}\\ 
	\hspace*{\fill}\today
\end{abstract}
\setcounter{page}{1}
\newpage