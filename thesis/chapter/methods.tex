\chapter{Methodology}

\section{Task}

The specific task we are trying to solve is one of drum instrument onset detection. For each timestep of an input spectrogram, we want to predict the probability of a drum onset happening.

\textcolor{red}{Move this piece to background and expand ADT information there?}

\section{Pipeline}

\subsection{Preprocessing}

\textcolor{red}{Now explain what we do to the data before prediction. Explain the preprocessing steps we do afterwards (normalization, etc).}

\textcolor{red}{And explain how we preprocess the labels (target widening, etc.).}

\subsection{Training}

\textcolor{red}{Mention the loss function used, and why we use this (BCEWithLogitsLoss).}

\textcolor{red}{Mention the computation of infrequency weights, i.e. how they are computed, why they are computed, the intuition into how they will help us...}

\subsection{Postprocessing}

\textcolor{red}{Mention how model outputs a "confidence in event happening" distribution, which we want to discretize into events.
I.e. explain Vogl's peak picking algorithm~\cite{vogl2018multiinstrumentdrumtranscription}.}

\section{Experiments}

\textcolor{red}{Here we mention the setups for each of the experiments.}

\textcolor{red}{Mention that we use RayTune to train, with PyTorch models. Mention that we only used RayTune's FIFOScheduler, and how for random search / grid search we used their built in parameter space functionality.}

\textcolor{red}{Mention that every single experiment was trained for at most 100 epochs, with a early stopping if validation loss didn't decrease within 10 epochs.
Mention the learning rate scheduler, where we reduce the learning rate by a factor of 5 if the model hasn't improved in the last 3 epochs.}

\textcolor{red}{Mention that we perform early stopping on the validation loss (and why, like the smooth nature, overfitting prevention, etc.), where as we store the best performing model based on the validation F1-score (due to this representing overall prediction performance).}

\textcolor{red}{Mention that every experiment is model selected using hold-out validation, and best model is chosen based on micro F1-score.}