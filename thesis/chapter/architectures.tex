\chapter{Architectures}\label{Architectures}

Choosing an suitable architecture is crucial for achieving strong performance in deep learning. The choice of model defines not only the capacity for learning complex patterns, but also the inductive biases that guide generalization. In the context of \gls{ADT}, these models must operate on time-frequency representations of audio and produce accuracte, frame-level onset predictions.

Each input to the model is a spectrogram of shape $(T \times D_{STFT})$, where $T$ is the sequence length (i.e., number of frames) and $D_{STFT}$ is the number of frequency bins. The corresponding output is a matrix of shape $(T \times C)$, where $C$ is the number of drum instruments being predicted.

In this thesis, I use log-magnitude spectrograms filtered with 12 logarithmically spaced, area-normalized filters per octave, spanning 20~Hz to 20,000~Hz. This results in $D_{STFT} = 84$ frequency bins. The task is to predict a common 5 class drum vocabulary, consisting of \acrfull{KD}, \acrfull{SD}, \acrfull{TT}, \acrfull{HH}, and \acrfull{CC+RC}, giving $C = 5$~\cite{zehren2024analyzingreducingsynthetictorealtransfer}.

While the input sequence length $T$ may vary, it is fixed to $T = 400$ for all experiments in this thesis, corresponding to 4 seconds of audio with 10~ms hop length. Thus, the models considered in this chapter operate with input and output shapes of $(T \times 84)$ and $(T \times 5)$, respectively.

\section{Recurrent Neural Network}

\glspl{RNN} are a foundational architecture for modeling sequential data and have demonstrated promising results on a wide range of audio-related tasks. Their core component is the \textit{recurrent unit}, which processes an input sequence one frame at a time while maintaining a hidden state that carries information from previous timesteps. This enables the model to capture temporal dependencies within the input.

To include information from both past and future frames, \glspl{RNN} can be extended to their \textit{bidirectional} variant, where one recurrent layer processes the sequence forward in time and another in reverse. This is particularly useful in tasks such as \gls{ADT}, where relevant auditory information may span multiple frames; for example, due to instrument timbre persisting after onset.

\begin{figure}[H]
    \centering
    \input{tikz/rnnspectrogram}
    \caption{Illustration of how bidirectional \glspl{RNN} include information from surrounding timesteps when predicting the current frame. The background represents a mock spectrogram of shape $(T \times D_{STFT})$, and the red boxes indicate the relative temporal influence on the middle frame. Box height reflects influence strength, gradually tapering off as distance from the center sequentially increases.}
    \label{RNNInfluenceFigure}
\end{figure}

Despite their effectiveness, traditional \glspl{RNN} suffer from the \textit{vanishing gradient problem}, which limits their ability to learn long-range dependencies. To address this, more advanced variants have been proposed, including the \gls{GRU} by Cho et al.~\cite{DBLP:conf/emnlp/ChoMGBBSB14} and the \gls{LSTM} by Hochreiter and Schmidhuber~\cite{10.1162/neco.1997.9.8.1735}.

Both \glspl{GRU} and \glspl{LSTM} have been successfully applied to \gls{ADT}-related tasks~\cite{Southall2016AutomaticDT, vogl2016recurrent, Vogl2017DrumTV, signals4040042}.

\subsection{Implementation}

I implemented the \gls{RNN} architecture using a stack of \glspl{BiRU}, followed by a framewise linear projection. These bidirectional recurrent layers extract and combine local temporal features at each time frame, allowing the model to consider both past and future context. The final linear layer then maps each resulting hidden state to onset probabilities for each of the five drum instruments. 

As part of the model search, I experimented with both bidirectional \glspl{GRU} and \glspl{LSTM} as the recurrent unit, treating this choice as a tunable hyperparameter. I also varied the number of layers $L$ and the hidden size $H$.

\begin{figure}[H]
    \centering
    \input{tikz/rnn}
    \caption{Model architecture of the \acrlong{RNN} used in this thesis.}
    \label{RNNFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $L$ & Number of layers      & \{2, 3, 4, 5, 6\} \\
        $H$ & Hidden size      & \{72, 144, 288, 576\} \\
        \gls{BiRU} & \acrlong{BiRU} & \{\gls{GRU}, \gls{LSTM}\}\\
    \end{tabular}
    \caption{Hyperparameters and their corresponding search spaces for training the \acrlong{RNN}.}
    \label{RNNHyperparams}
\end{table}

\section{Convolutional Neural Network}

Although spectrograms are often processed as time sequences, we've seen that they also naturally resemble images, with time and frequency forming the horizontal and vertical axes. This makes them well-suited for \glspl{CNN}, which are designed to extract local patterns in data.

A \gls{CNN} operates by applying learnable filters, called \textit{kernels}, across the input. These filters aggregate spatially local information, allowing each output unit to incorporate context from its surrounding region. When applied to spectrograms, convolutional layers enable the model to detect patterns in time-frequency space. This makes them a strong candidate for tasks like \gls{ADT}, where relevant features often span multiple adjacent time frames and frequency bins.

\glspl{CNN} have also been shown to perform well in \gls{ADT}, likely because this contextual information helps the model more easily learn the characteristics of instrument onsets. They are also relatively efficient to train and run, which may also help explain their reasonable performance~\cite{Vogl2017DrumTV}.

\begin{figure}[H]
    \centering
    \input{tikz/cnnspectrogram}
    \caption{Illustration of how \glspl{CNN} use a fixed-size receptive field to incorporate local context from surrounding time frames. The background represents a mock spectrogram. The red shaded region marks the center time frame being predicted, and the red boxes indicate the receptive field, the neighboring frames that influence the prediction.}
    \label{CNNInfluenceFigure}
\end{figure}

\subsection{Implementation}

My \gls{CNN} architecure begins with $I$ initial convolutional blocks, each designed to preserve the temporal resolution of the input, ensuring that the output retains the same sequence length $T$. Within these blocks, the number of kernels $K_i$ increases with block depth $i$, using $K = \{32, 64, 96\}$ to enable deeper layers to capture more complex time–frequency ptterns. This kernel progression is fixed (i.e., not treated as a hyperparameter), and follows a common design convention of increasing the filter count with depth~\cite{simonyan2014very}.

These convolutional blocks iteratively combine local features from the spectrogram, forming a richer internal representation of the input. This representation is then passed through $L$ fully connected layers, which project the features into an $H$-dimensional latent space for final interpretation. 

Each convolutional and fully connected layer is followed by a \gls{ReLU} activation function. Finally, an output layer computes onset probabilities for each of the five drum instruments.

\begin{figure}[H]
    \centering
    \input{tikz/cnn}
    \caption{Model architecture of the \acrlong{CNN} used in this thesis.}
    \label{CNNFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $I$ & Number of convolutions & \{1, 2, 3\}\\
        $L$ & Number of layers      & \{2, 3, 4\} \\
        $H$ & Hidden size      & \{72, 144, 288, 576\} \\
    \end{tabular}
    \caption{Hyperparameters and their corresponding search spaces for training the \acrlong{CNN}.}
    \label{CNNHyperparams}
\end{table}

\section[Convolutional RNN]{Convolutional Recurrent Neural Network}

The strengths of the recurrent layers and convolutions are not mutually exclusive. Theoretically, they can harmonize when combined into a unified architecture, the \gls{CRNN}, where each component complements the other.

Intuitively, the ability of \glspl{CNN} to extract local time–frequency patterns from spectrograms, together with the ability of \glspl{RNN} to model temporal dependencies, makes this combination particularly well-suited for \gls{ADT}. This merging of contextual representation and cross-timestep memory has shown promising results in prior work~\cite{Vogl2017DrumTV, vogl2018multiinstrumentdrumtranscription, signals4040042}.

\subsection{Implementation}

I implemented the \gls{CRNN} using a fixed stack of $I = 2$ initial convolutional blocks, a setup inspired by prior work in \gls{ADT}~\cite{Vogl2017DrumTV, signals4040042}. These convolutional blocks use an increasing number of kernels $K = \{32, 64\}$, allowing deeper layers to extract more complex time–frequency patterns. As with the pure \gls{CNN}, the convolutions preserve the input's temporal resolution.

The resulting convolutional output is passed to a \gls{BiRU}, which models temporal dependencies across frames. As in the \gls{RNN} architecture, I experimented with both \glspl{GRU} and \glspl{LSTM}. Each timestep's hidden state is then passed into the final linear layer, which computes and outputs onset probabilities for each of the five drum instruments.

\begin{figure}[H]
    \centering
    \input{tikz/crnn}
    \caption{Model architecture of the \acrlong{CRNN} used in this thesis.}
    \label{CRNNFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $L$ & Number of layers      & \{2, 3, 4, 5\} \\
        $H$ & Hidden size      & \{72, 144, 288, 576\} \\
        \gls{BiRU} & \acrlong{BiRU} & \{\gls{GRU}, \gls{LSTM}\}\\
    \end{tabular}
    \caption{Hyperparameters and their corresponding search spaces for training the \acrlong{CRNN}.}
    \label{CRNNHyperparams}
\end{table}

\section{Convolutional Transformer}

Google's "Attention Is All You Need"~\cite{NIPS2017_3f5ee243} made headway in regards to sequence prediction. It introduced the \textit{Attention} layer, making a model capable of learning to \textit{attend} to different elements in a sequence, and learning the relationship between them. Models dropping the recurrent units in favour of blocks containing attention layers (so-called transformer blocks) are called \textit{transformers}.

As mentioned, the \gls{RNN} displays difficulty in sustaining long range dependencies through its hidden state, and information further away tends to become attenuated. The attention layer solves this by allowing each element to individually attend to each other element in the sequence separately. Intuitively, it allows each element to \textit{"intelligently"} pick and choose where it wants to look, and what elements it wants to be influenced by. This stands in contrast to the recurrent units, where each element has to learn and predict what information about itself other elements could find useful, and \textit{"remembering"} that, adding it to the hidden state.

\begin{figure}[H]
    \centering
    \input{tikz/ctspectrogram}
    \caption{An example of how attention layers allow for attentive influence from from other timeframes spanning large distances. The background is a spectrogram, and the red boxes represent influence on the middle timeframe. The height of the box help visualize influence strength.}
    \label{CTInfluenceFigure}
\end{figure}


Recently, the attention layers have shown great success in \gls{AMT} and \gls{ADT} tasks, in some cases proving superior to the \gls{RNN}~\cite{gardner2022mt3multitaskmultitrackmusic, chang2024yourmt3+, zehren2024analyzingreducingsynthetictorealtransfer}.

Simply replacing the recurrent layers with transformer blocks could allow our model to reap the reward by increasing its ability to understand sequences, while keeping the previously gotten gains from the convolutional layer. Both inside and outside of \gls{ADT}, combining convolutional layers with transformers has seemed beneficial~\cite{zehren2024analyzingreducingsynthetictorealtransfer, gulati2020conformerconvolutionaugmentedtransformerspeech}.

\subsection{Implementation}

Similar to the \gls{CRNN}, an initial fixed-size convolutional block with $I = 2$ is used, with an increasing number of kernels $C = \{32, 64\}$. Then, we project the resulting latent space into a separate, lower dimensional embedding space with dimension $D_e$, before combining it with a sinusoidal positional encoding. 

Following this, the model contains $L$ standard pre-layer normalization transformer block, as these recently have been shown to be more stable during learning than the post-layer versions~\cite{pmlr-v119-xiong20b}. These transformer blocks contain multi-head self-attention layers with $H$ number of heads. Note that the first layer of the feedforward layer inside these transformer blocks uses the \gls{GELU} activation function due to it being the standard within transformers and for its possible performance improvements over \gls{ReLU}~\cite{devlin-etal-2019-bert, hendrycks2023gaussianerrorlinearunits}. Lastly, the linear layer outputs onset probabilities.

\begin{figure}[H]
    \centering
    \input{tikz/ct}
    \caption{Model architecture of the Convolutional Transformer used in this thesis.}
    \label{CTFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $H$ & Number of heads     & \{2, 4, 6, 8\} \\
        $L$ & Number of layers      & \{2, 4, 6, 8, 10\} \\
        $D_e$ & Embedding dimension      & \{72, 144, 288, 576\} \\
    \end{tabular}
    \caption{Hyperparameters and their corresponding search spaces for training the Convolutional Transformer.}
    \label{CTHyperparams}
\end{table}

\section{Vision Transformer}

Introducing the Transformer raises a question. Would it be possible for an architecture to be comprised of solely attention layers, removing convolutions and breaking this need for a mixed convolutional-transformer architecture. This question was answered by Google's "An Image Is Worth 16x16 Words"~\cite{dosovitskiy2021imageworth16x16words}, introducing the Vision Transformer.

The Vision Transformer was created to tackle image recognition tasks, though it has been applied to audio classification, displaying great performance on both~\cite{dosovitskiy2021imageworth16x16words, gong2021astaudiospectrogramtransformer}. However, application of the Vision Transformer on an \gls{ADT} task is a novel approach.

It is worth to note that Vision Transformers have been shown to display excellent performance, however they usually need more significantly more data than other architectures to function optimally~\cite{dosovitskiy2021imageworth16x16words}.

\subsection{Patch Embedding}

A key component of the Vision Transformer is the creation of a patch embedding. First, we split the input image into different non-overlapping patches, and flatten them. These patches are linearly projected into a latent space, and a positional encoding is added to retain positional information. This resulting sequence of patches is what is referred to as a patch embedding.

We usually say that patch embeddings eliminate the use of convolutions in the Vision Transformer. However, the actual implementation of splitting the image into patches and linearly projecting each patch are usually implemented with a single 2D convolutional layer. Note that it is a linear projection, meaning the convolutional layer is absent of an activation function.

\begin{figure}[H]
    \centering
    %\input{tikz/patchembedding}
    \includegraphics[trim=0 0 0 116, clip, scale=0.7]{figures/patchembedding.png}
    \caption{The creation of a sequence of patch embeddings from an input spectrogram. The spectrogram is split into several overlapping or non-overlapping patches. These are linearly projected into a sequence of 1 dimensional patch embeddings. Each patch embedding also has a learnable positional embedding added to it. Here it is also represented with an initial \acrfull{CLS}, which often is the case for image classification tasks~\cite{gong2021astaudiospectrogramtransformer}.}
    \label{PatchEmbeddingFigure}
\end{figure}

\subsection{Architecture Modifications}

Originally, the Vision Transformer has been used on classification tasks where the output is not a sequence. \gls{ADT} is a sequence labeling task, and requires that the output is a sequence, where the time dimension matches the size of the input.

This can be solved by treating a group of patches from the patch embedding together as a timeframe, as long as we ensure that there the number of timeframes are a factor of the number of patches. Then the output of the Vision Transformer could be construed to match our intended output sequence. This also voids the need of an additional \gls{CLS}, which in classification tasks is transformed to represent the whole input image, later being used to output the class prediction~\cite{dosovitskiy2021imageworth16x16words}

\subsection{Implementation}

Initally, we transform the input spectrogram into a ($T \times D_e$) patch embedding. The Convolutional layer splits the spectrogram into ($T \times D_e / P \times P$) different patches. Added to these are a ($D_e / P \times P$) learnable positional embedding, providing positional information to each patch. These are then permuted and flattened, transforming them into our final patch embedding.

Afterwards, we combine it with a sinusoidal positional encoding, and successively apply $L$ transformer blocks with number of heads $H$, identical in structure with those used in the Convolutional Transformer. Lastly, the linear layer outputs onset probabilities.

\begin{figure}[H]
    \hspace*{-0.5cm}
    \centering
    \input{tikz/vit}
    \caption{Model architecture of the Vision Transformer used in this thesis.}
    \label{ViTFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $P$ & Patch height      & \{7, 14, 21\} \\
        $H$ & Number of heads     & \{2, 4, 6, 8\} \\
        $L$ & Number of layers      & \{2, 4, 6, 8, 10\} \\
        $D_e$ & Embedding dimension      & \{72, 144, 288, 576\} \\
    \end{tabular}
    \caption{Hyperparameters and their corresponding search spaces for training the Vision Transformer.}
    \label{ViTHyperparams}
\end{table}