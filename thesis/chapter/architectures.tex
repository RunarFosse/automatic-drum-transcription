\chapter{Architectures}\label{Architectures}

Finding a suitable architecture is a vital step in creating a well-performant deep learning model. By leveraging different techniques, we balance introduction of inductive biases and possibility for complexity, which hopefully can help us end up with a generalizing model.

The models using these architectures will be applied onto spectrograms. These spectrograms have a size $(T \times D_{STFT})$, where $T$ is the sequence length, or number of frames, in the spectrogram, and $D_{STFT}$ denoting the number of frequency bins. The output should be, as mentioned previously, a labelling of this input sequence, having a size of $(T \times C)$, where $T$ is the same sequence length as the input sequence, and $C$ denoting the number of classes, equal to the number of drum instruments.

The spectrograms we are working with are logarithimically filtered log-magnitude spectrograms with 12 normalized logarithmically spaced filters per octave from 20 to 20,000H. This results in $D_{STFT} = 84$ frequency bins. Our intention is to train a model suitable at predicting the largest common vocabulary in \gls{ADT}, namely the 5-class vocabulary consisting of \acrfull{KD}, \acrfull{SD}, \acrfull{TT}, \acrfull{HH}, \acrfull{CC+RC}~\cite{zehren2024analyzingreducingsynthetictorealtransfer}. In other words, we have that $C = 5$.
Although the sequence length $T$ is an important shape, determining temporal resolution of the input and output, it has the unique ability to vary, allowing for differently sized inputs and outputs on the same models. For all intents and purposes however, this thesis fixes $T = 400$, representing 4 seconds of 10ms sized frames.

In summary, we want our models to have an input size of $(T \times 84)$, and an output size of $(T \times 5)$, where $T$ could vary.

\section{Recurrent Neural Network}

The \gls{RNN} is a standard architecture when it comes prediction on sequence data. It has been tried and tested, showing promising results for audio tasks.

The fundamental building block for \glspl{RNN} is the \textit{recurrent unit}. It iterates the whole input sequence, storing information from previous timesteps in a form of memory, through maintenance of a \textit{hidden state}. This can be extended to gaining information about future timesteps by using \textit{bidirectional} versions. In this way, prediction on current timesteps are affected by the information from surrounding timesteps. This is relevant in tasks such as \gls{ADT} as auditory information usually spreads over several timesteps, e.g. the timbre of an instrument event lingering after onset.

\begin{figure}[H]
    \centering
    \input{tikz/rnnspectrogram}
    \caption{An example of how bidirectional \glspl{RNN} allow for adjacent timesteps to influence the prediction of the current timestep. The background is a spectrogram, and the red boxes represent influence in the middle timeframe. The height of the box help visualize influence strength.}
    \label{RNNInfluenceFigure}
\end{figure}

However, traditional \glspl{RNN} suffer from the \textit{vanishing gradient problem} due to a timestep's influence diminishing with distance, making \textit{long range dependencies} harder to learn. Different architectures have been developed to try to overcome these issues, such as the \gls{GRU} by Cho et al.~\cite{DBLP:conf/emnlp/ChoMGBBSB14}, and \gls{LSTM} by Hochreiter and Schmidhuber~\cite{10.1162/neco.1997.9.8.1735}.

It has been shown that \glspl{GRU} and \glspl{LSTM} are capable of learning \gls{ADT} related tasks, and is therefore in interest to comparatively measure how their efficiency stands in regards to other architectures~\cite{Southall2016AutomaticDT, vogl2016recurrent, Vogl2017DrumTV, signals4040042}.

\subsection{Implementation}

Our \gls{RNN} architecture consists of several bidirectional recurrent units, ending in a framewise linear layer. For the \gls{BiRU}, we train either a \gls{GRU} or an \gls{LSTM} model as hyperparameters, in addition to search over number of layers $L$ and hidden size $H$, selecting the one with best performance. At last, we have a linear layer, outputting onset probabilities for each of the drums per timeframe.

\begin{figure}[H]
    \centering
    \input{tikz/rnn}
    \caption{RNN architecture structure.}
    \label{RNNFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $L$ & Number of layers      & \{2, 3, 4, 5, 6\} \\
        $H$ & Hidden size      & \{72, 144, 288, 576\} \\
        \gls{BiRU} & \acrlong{BiRU} & \{\gls{GRU}, \gls{LSTM}\}\\
    \end{tabular}
    \caption{The different hyperparameters and their respective values tuned to train the Recurrent Neural Network.}
    \label{RNNHyperparams}
\end{table}

\section{Convolutional Neural Network}

We've mentioned that spectrograms can be treated as images. Therefore it would make sense to try an image focused approach, by utilizing \textit{convolutions}. By applying convolutional layers, each timestep gets access to information around itself, a \textit{context}. These convolutional layers make up the primary building blocks for the \gls{CNN}.

\begin{figure}[H]
    \centering
    \input{tikz/cnnspectrogram}
    \caption{An example of how \glspl{CNN} allow for a fixed context of influence from neighbouring timesteps when predicting each timestep. The background is a spectrogram, and the red boxes represent influence in the middle timeframe, being described by the red shaded region.}
    \label{CNNInfluenceFigure}
\end{figure}

\glspl{CNN} have been shown to give reasonable performance within \gls{ADT}. This could be due to contextual information being important for identifying instrument onsets, and making learning easier for our models~\cite{Vogl2017DrumTV}.

\subsection{Implementation}

Our \gls{CNN} architecure consists of $I$ initial convolutional blocks. Inside this block, convolutional layers have an increasing number of kernels $C = \{32, 64, 96\}$, intuitively leading to an increase in complexity along with depth. Note that this is not a hyperparameter, but just denotes that we increase the amount of kernels the deeper into the convolutional block we go. Then we have a varying amount of fully connected layers $L$, projecting into a latent space sized $H$. Both the convolutional and fully connected layers are followed by a \gls{ReLU} activation function. Lastly, an output layer computed each instrument's onset probabilities.

\begin{figure}[H]
    \centering
    \input{tikz/cnn}
    \caption{CNN architecture structure.}
    \label{CNNFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $I$ & Number of convolutions & \{1, 2, 3\}\\
        $L$ & Number of layers      & \{2, 3, 4\} \\
        $H$ & Hidden size      & \{72, 144, 288, 576\} \\
    \end{tabular}
    \caption{The different hyperparameters and their respective values tuned to train the Convolutional Neural Network.}
    \label{CNNHyperparams}
\end{table}

\section[Convolutional RNN]{Convolutional Recurrent Neural Network}

The previous features, recurrent layers and convolutions, are not mutually exclusive. Theoretically they can harmonize together, complementing eachother for easier learning. This results in the \gls{CRNN} architecture.

Intuitively, the \glspl{CNN} ability to process images like the spectrogram, together with the \glspl{RNN} ability to understand temporal sequence data should prove beneficial for \gls{ADT} tasks. And indeed, this combination of cross-timestep memory and contextual data representation has shown to be insightful~\cite{Vogl2017DrumTV, vogl2018multiinstrumentdrumtranscription, signals4040042}.

\subsection{Implementation}

We begin with a fixed-size convolutional block with $I = 2$, as used by several \gls{ADT} authors~\cite{Vogl2017DrumTV, signals4040042}. Following the \gls{CNN}, it has an increasing number of kernels $C = \{32, 64\}$ the deeper into the convolutions we go. We then, similar to the \gls{RNN}, have a \gls{BiRU} layer of either a \gls{GRU} or \gls{LSTM}, with number of layers $L \in \{2, 3, 4, 5\}$ and hidden size $H \in \{72, 144, 288\}$. Output probabilities are then computed through the final linear layer.

\begin{figure}[H]
    \centering
    \input{tikz/crnn}
    \caption{Convolutional RNN architecture structure.}
    \label{CRNNFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $L$ & Number of layers      & \{2, 3, 4, 5\} \\
        $H$ & Hidden size      & \{72, 144, 288, 576\} \\
        \gls{BiRU} & \acrlong{BiRU} & \{\gls{GRU}, \gls{LSTM}\}\\
    \end{tabular}
    \caption{The different hyperparameters and their respective values tuned to train the Convolutional Recurrent Neural Network.}
    \label{CRNNHyperparams}
\end{table}

\section{Convolutional Transformer}

Google's "Attention Is All You Need"~\cite{NIPS2017_3f5ee243} made headway in regards to sequence prediction. It introduced the \textit{Attention} layer, making a model capable of learning to \textit{attend} to different elements in a sequence, and learning the relationship between them. Models dropping the recurrent units in favour of blocks containing attention layers (so-called transformer blocks) are called \textit{transformers}.

As mentioned, the \gls{RNN} displays difficulty in sustaining long range dependencies through its hidden state, and information further away tends to become attenuated. The attention layer solves this by allowing each element to individually attend to each other element in the sequence separately. Intuitively, it allows each element to \textit{"intelligently"} pick and choose where it wants to look, and what elements it wants to be influenced by. This stands in contrast to the recurrent units, where each element has to learn and predict what information about itself other elements could find useful, and \textit{"remembering"} that, adding it to the hidden state.

\begin{figure}[H]
    \centering
    \input{tikz/ctspectrogram}
    \caption{An example of how attention layers allow for attentive influence from from other timeframes spanning large distances. The background is a spectrogram, and the red boxes represent influence in the middle timeframe. The height of the box help visualize influence strength.}
    \label{CTInfluenceFigure}
\end{figure}


Recently, the attention layers have shown great success in \gls{AMT} and \gls{ADT} tasks, in some cases proving superior to the \gls{RNN}~\cite{gardner2022mt3multitaskmultitrackmusic, chang2024yourmt3+, zehren2024analyzingreducingsynthetictorealtransfer}.

Simply replacing the recurrent layers with transformer blocks could allow our model to reap the reward by increasing its ability to understand sequences, while keeping the previously gotten gains from the convolutional layer. Both inside and outside of \gls{ADT}, combining convolutional layers with transformers has seemed beneficial~\cite{zehren2024analyzingreducingsynthetictorealtransfer, gulati2020conformerconvolutionaugmentedtransformerspeech}.

\subsection{Implementation}

Similar to the \gls{CRNN}, an initial fixed-size convolutional block with $I = 2$ is used, with an increasing number of kernels $C = \{32, 64\}$. Then, we project the resulting latent space into a separate, lower dimensional embedding space with dimension $D_e$, before combining it with a sinusoidal positional encoding. 

Following this, the model contains $L$ standard pre-layer normalization transformer block, as these recently have been shown to be more stable during learning than the post-layer versions~\cite{pmlr-v119-xiong20b}. These transformer blocks contain multi-head self-attention layers with $H$ number of heads. Note that the first layer of the feedforward layer inside these transformer blocks uses the \gls{GELU} activation function due to it being the standard within transformers and for its possible performance improvements over \gls{ReLU}~\cite{devlin-etal-2019-bert, hendrycks2023gaussianerrorlinearunits}. Lastly, the linear layer outputs onset probabilities.

\begin{figure}[H]
    \centering
    \input{tikz/ct}
    \caption{Convolutional Transformer architecture structure.}
    \label{CTFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $H$ & Number of heads     & \{2, 4, 6, 8\} \\
        $L$ & Number of layers      & \{2, 4, 6, 8, 10\} \\
        $D_e$ & Embedding dimension      & \{72, 144, 288, 576\} \\
    \end{tabular}
    \caption{The different hyperparameters and their respective values tuned to train the Convolutional Transformer.}
    \label{CTHyperparams}
\end{table}

\section{Vision Transformer}

Introducing the Transformer raises a question. Would it be possible for an architecture to be comprised of solely attention layers, removing convolutions and breaking this need for a mixed convolutional-transformer architecture. This question was answered by Google's "An Image Is Worth 16x16 Words"~\cite{dosovitskiy2021imageworth16x16words}, introducing the Vision Transformer.

The Vision Transformer was created to tackle image recognition tasks, though it has been applied to audio classification, displaying great performance on both~\cite{dosovitskiy2021imageworth16x16words, gong2021astaudiospectrogramtransformer}. However, application of the Vision Transformer on an \gls{ADT} task is a novel approach.

It is worth to note that Vision Transformers have been shown to display excellent performance, however they usually need more significantly more data than other architectures to function optimally~\cite{dosovitskiy2021imageworth16x16words}.

\subsection{Patch Embedding}

A key component of the Vision Transformer is the creation of a patch embedding. First, we split the input image into different non-overlapping patches, and flatten them. These patches are linearly projected into a latent space, and a positional encoding is added to retain positional information. This resulting sequence of patches is what is referred to as a patch embedding.

We usually say that patch embeddings eliminate the use of convolutions in the Vision Transformer. However, the actual implementation of splitting the image into patches and linearly projecting each patch are usually implemented with a single 2D convolutional layer. Note that it is a linear projection, meaning the convolutional layer is absent of an activation function.

\begin{figure}[H]
    \centering
    %\input{tikz/patchembedding}
    \includegraphics[trim=0 0 0 116, clip, scale=0.7]{figures/patchembedding.png}
    \caption{The creation of a sequence of patch embeddings from an input spectrogram. The spectrogram is split into several overlapping or non-overlapping patches. These are linearly projected into a sequence of 1 dimensional patch embeddings. Each patch embedding also has a learnable positional embedding added to it. Here it is also represented with an initial \acrfull{CLS}, which often is the case for image classification tasks~\cite{gong2021astaudiospectrogramtransformer}.}
    \label{PatchEmbeddingFigure}
\end{figure}

\subsection{Architecture Modifications}

Originally, the Vision Transformer has been used on classification tasks where the output is not a sequence. \gls{ADT} is a sequence labeling task, and requires that the output is a sequence, where the time dimension matches the size of the input.

This can be solved by treating a group of patches from the patch embedding together as a timeframe, as long as we ensure that there the number of timeframes are a factor of the number of patches. Then the output of the Vision Transformer could be construed to match our intended output sequence. This also voids the need of an additional \gls{CLS}, which in classification tasks is transformed to represent the whole input image, later being used to output the class prediction~\cite{dosovitskiy2021imageworth16x16words}

\subsection{Implementation}

Initally, we transform the input spectrogram into a ($T \times D_e$) patch embedding. The Convolutional layer splits the spectrogram into ($T \times D_e / P \times P$) different patches. Added to these are a ($D_e / P \times P$) learnable positional embedding, providing positional information to each patch. These are then permuted and flattened, transforming them into our final patch embedding.

Afterwards, we combine it with a sinusoidal positional encoding, and successively apply $L$ transformer blocks with number of heads $H$, identical in structure with those used in the Convolutional Transformer. Lastly, the linear layer outputs onset probabilities.

\begin{figure}[H]
    \hspace*{-0.5cm}
    \centering
    \input{tikz/vit}
    \caption{Vision Transformer architecture structure.}
    \label{ViTFigure}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lr|c}
        \multicolumn{2}{c|}{Hyperparameter} & Values       \\
        \hline
        $P$ & Patch height      & \{7, 14, 21\} \\
        $H$ & Number of heads     & \{2, 4, 6, 8\} \\
        $L$ & Number of layers      & \{2, 4, 6, 8, 10\} \\
        $D_e$ & Embedding dimension      & \{72, 144, 288, 576\} \\
    \end{tabular}
    \caption{The different hyperparameters and their respective values tuned to train the Vision Transformer.}
    \label{ViTHyperparams}
\end{table}