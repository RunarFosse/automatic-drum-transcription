\chapter{Dataset Study}\label{Study2}

This study's main purpose is to figure out how each dataset, and their characteristics, combined can influence a model's performance, both on- and \gls{OOD}. This could help us figure out how to utilize and combine current datasets, or how to intelligently construct future datasets, to optimize models' performance and generalization ability for \acrfull{ADT} and \acrfull{DTM}.

\section{Methodology}

We train convolutional recurrent neural networks, the best performing architecture from the first study, over several different combinations of the first four datasets ENST+MDB, E-GMD, Slakh, and ADTOF-YT. Datasets are combined as a union, where one epoch over the combined dataset would be equivalent to one epoch of both datasets separately. Each model is also evaluated on the remaining datasets, in addition to the SADTP dataset.

To get a comprehensive overview over how the different datasets could complement or contrast eachother's performances, without doing redundant experiments with diminishing returns, we select and train on a subset of all possible dataset combination. More specifically, we select 10 different combinations of datasets in such a way that we could extract valuable information about how their characteristics affect the resulting model's generalization ability.

\section{Results}

\begin{table}[H]
    \centering
    \hspace*{-1.0cm}
    \resizebox{1.13\linewidth}{!}{%
    \begin{tabular}{l|ccccc}
        Training Datasets & ENST+MDB & E-GMD & Slakh & ADTOF-YT & SADTP      \\
        \hline
        ENST+MDB	& 0.81	& \cellcolor{blue!5} 0.59	& \cellcolor{blue!5} 0.53	& \cellcolor{blue!5} 0.60	& \cellcolor{blue!5} 0.42 \\
        E-GMD	& \cellcolor{blue!5} 0.55	& \textbf{0.90}	& \cellcolor{blue!5} 0.44	& \cellcolor{blue!5} 0.42	& \cellcolor{blue!5} 0.28 \\
        Slakh	& \cellcolor{blue!5} 0.80	& \cellcolor{blue!5} 0.73	& 0.90	& \cellcolor{blue!5} 0.59	& \cellcolor{blue!5} 0.48 \\
        ADTOF-YT	& \cellcolor{blue!5} 0.84	& \cellcolor{blue!5} 0.69	& \cellcolor{blue!5} \underline{0.65}	& 0.93	& \cellcolor{blue!5} 0.60 \\
        \hline
        ENST+MDB + Slakh	& 0.84	& \cellcolor{blue!5} 0.73	& 0.90	& \cellcolor{blue!5} \underline{0.63}	& \cellcolor{blue!5} 0.48 \\
        ENST+MDB + ADTOF-YT	& 0.86	& \cellcolor{blue!5} 0.70 	& \cellcolor{blue!5} 0.63	& 0.94	& \cellcolor{blue!5} \textbf{\underline{0.62}} \\
        Slakh + ADTOF-YT	& \cellcolor{blue!5} \underline{0.86}	& \cellcolor{blue!5} 0.72	& \textbf{0.90}	& \textbf{0.97}	& \cellcolor{blue!5} \textbf{\underline{0.62}} \\
        \hline
        ENST+MDB + Slakh + ADTOF-YT	& \textbf{0.88}	& \cellcolor{blue!5} \underline{0.74}	& 0.89	& 0.95	& \cellcolor{blue!5} 0.61 \\
        E-GMD + Slakh + ADTOF-YT	& \cellcolor{blue!5} 0.85	& \textbf{0.90} 	& 0.89	& 0.93	& \cellcolor{blue!5} 0.61 \\
        \hline
        ENST+MDB + E-GMD + Slakh + ADTOF-YT	& 0.87	& 0.89	& \textbf{0.90} 	& 0.93	& \cellcolor{blue!5} \textbf{\underline{0.62}} \\
    \end{tabular}%
    }
    \caption{The Micro F1-score for a convolutional recurrent neural network trained over different combination of datasets, and tested on all datasets. The performances which are \textbf{bolded} represent the highest overall F1-score for the given dataset. Cells which are \colorbox{blue!10}{coloured light blue} represent \gls{OOD} evaluations. Values which are \underline{underlined} represent the highest possible F1-score of \gls{OOD} evaluations for the respective dataset.}
    \label{DatasetResultsTable}
\end{table}

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{l|cccc}
        Model & ENST+MDB & E-GMD & Slakh & ADTOF-YT      \\
        \hline
        Slakh + ADTOF-YT & \cellcolor{blue!5} 0.86	& \cellcolor{blue!5} 0.72 & \textbf{0.90}	& \textbf{0.97} \\
        ENST+MDB + Slakh + ADTOF-YT & \textbf{0.88}	& 0.74 & \cellcolor{blue!5} 0.89	& 0.95 \\
        E-GMD + Slakh + ADTOF-YT & \cellcolor{blue!5} 0.85 & \textbf{0.90} & 0.89 & 0.93 \\
        \hline
        OaF-Drums~\cite{callender2020improvingperceptualqualitydrum} & \cellcolor{blue!5} 0.77/$\times$* & 0.83 & $\times$ & $\times$ \\
        \hline
        MT3 (mixture)~\cite{gardner2022mt3multitaskmultitrackmusic} & $\times$ & $\times$ & 0.76 & $\times$ \\
        \hline
        YPTF.MoE+M~\cite{chang2024yourmt3+} & 0.87/$\times$** & $\times$ & 0.85 & $\times$ \\
        \hline
        ADTOF-RGW + ADTOF-YT~\cite{signals4040042} & \cellcolor{blue!5}0.78/0.81*** & $\times$ & $\times$ & 0.85 \\
    \end{tabular}
    \caption{The Micro F1-score of selectively chosen convolutional recurrent neural networks from this dataset study, compared with best performing models from other literature over the different test sets. The performances which are \textbf{bolded} represent the highest overall F1-score for the given dataset for all compared models. Cells which are \colorbox{blue!10}{coloured light blue} represent \gls{OOD} evaluations.
    (*) Callender et. al only tests ENST-Drums, and strictly on isolated drum stems handling it as a \gls{DTD} task, on a different test split than ours~\cite{callender2020improvingperceptualqualitydrum}.
    (**) Chang et. al only tests ENST-Drums and tests on a different test split than ours~\cite{chang2024yourmt3+}.
    (***) Zehren et. al tests on ENST-Drums and MDB-Drums separately and tests on different test splits than ours~\cite{signals4040042}.
    }
    \label{DatasetComparisonTable}
\end{table}

\textcolor{red}{Add E-GMD to other literature and discuss it in discussion.}

\section{Discussion}

The results from the dataset study, summarized in Table \ref{DatasetResultsTable} highlight important insights regarding how dataset composition could affect and impact model generalization. These results clearly demonstrate that strategically combining \gls{ADT} datasets improve both on- and \acrfull{OOD} generalization.

Firstly, it is evident that each dataset exhibits great performance on test splits which distribution overlaps with their own training dataset, but a lesser performance when testing \gls{OOD}. There are several hypotheses one could propose for why this is the case, however one plausible one is that the datasets are characteristically varied enough in such a way that they each contrast eachother. This could give rise to this generalization penalty we see and explains the drop in performance. This generalization penalty also seem to be very uniform for different datasets, except one (E-GMD) which is talked about in the following paragraph. The best performing \gls{OOD} evaluations all lie in the Micro F1-score range of $0.6$ to $0.7$, except for the smallest dataset (ENST+MDB) which displays an F1-score of around $0.87$. Interestingly, the best performing \gls{OOD} evaluation for ENST+MDB is significantly close to its best performance overall. As we know, in contrast to the other datasets ENST+MDB consist of high quality, really precise \gls{DTM} data. A hypothesis is that this transcription quality of ENST+MDB more easily show the strength of the other model's, which might be hidden in the lesser quality of other test splits.

Another important observation, as mentioned, is that of E-GMD. As known, this dataset differs from the others by being a \acrfull{DTD} dataset, instead of a \gls{DTM} one. Despite its large size, it is the worst at generalizing to the others, and it is the worst performing model on all other datasets. However, it is worth to note that it also displays the best performance on itself. This helps reinforcing the differences between the tasks of \gls{DTD} and \gls{DTM}, showing how model's trained on differently tasked datasets are not directly applicable interchangeably. 

Building on this information we could inspect the opposite relationship. Notably, the other model's \gls{OOD} evaluation on E-GMD is comparable to that of their \gls{OOD} evaluations on themselves. This also helps strengthen the hypothesis that these tasks, \gls{DTD} and \gls{DTM}, display a complexity hierarchy where one seems to build upon the other. In other words, \gls{DTM} might be a more difficult transcription task than \gls{DTD}, but in return, a model trained for \gls{DTM} might be sufficient for \gls{DTD} tasks, and exhibits a zero-shot generalization ability which does not hold for the opposite.

One results that might merit discussion is the correlation between high ADTOF-YT performance and high SADTP performance. A high F1-score in the ADTOF-YT test split is accompanied by a relatively high \gls{OOD} F1-score for SADTP. Due to the crowdsourced nature of ADTOF-YT, and the SADTP consisting of contemporary and public music tracks, there might exist some overlap in their distribution which could explain this behaviour. This could also be explained by ADTOF-YT being a dataset containing characteristics which, when trained on, give the models a better ability at generalizing onto other \gls{ADT} datasets, as these models trained on ADTOF-YT also perform well \gls{OOD} on the other datasets.\textcolor{red}{Maybe show correlation graph?}.

An insightful observation is that the best performance of all \gls{DTM} datasets from the architecture study, sees themselves beat by models trained on a combination of datasets. This is an important observation and shows that model performance often is enhanced by expanding the dataset size and variation. However, building upon what we discussed in the last paragraph, it is important to understand the differences between \gls{DTD} and \gls{DTM} datasets. As we observe, the best Micro F1-score for \gls{DTD} dataset comes from training data solely of \gls{DTD} data. The same holds the other way, in that the best performances on \gls{DTM} datasets happen when solely trained on \gls{DTM} data.

Secondly, take a look at the model trained on the largest amount of data, the combination of all datasets. This model exhibits superior performance across the board, however interestingly it does not achieve the highest Micro F1-score on any, though being very close. This strengthens the hypothesis that expanding the amount data will heighten generalization ability for a given model, and agrees with the current consensus that training dataset size is vital to achieving an optimally generalizing model~\cite{signals4040042, 9747048}. For future works it would be interesting to analyze how valuable data augmentation would be for \gls{ADT}, and if it affects generalization positively.

Lastly, in Table \ref{DatasetComparisonTable} we compare our best performing model's with other literature and giving remarkable insight. Note that not much literature exist with models evaluated on thsee datasets. Gardner et. al's MT3~\cite{gardner2022mt3multitaskmultitrackmusic} and Chang et. al's YPTF.MOE+M~\cite{chang2024yourmt3+} are general \gls{AMT} models, predicting several different instruments in addition to drums. They also present using the Offset F1-score terminology, equivalent to our Micro F1-score (also used by Zehren et. al~\cite{signals4040042}). Zehren et. al's ADTOF-RGW + ADTOF-YT is a \gls{DTM} model, similar to that of ours. Observably, we note that our model's outperform the others' by a singificant amount on both the Slakh and ADTOF-YT datasets. Due to the splits and datasets being a bit different for ENST+MDB, a direct comparison is a bit more difficult, but one could reason that our model's performance is comparable to that of YPTF.MOE+M~\cite{chang2024yourmt3+}. By comparing with results from other literature, we can put our model's performance into perspective and strengthens the argument that our combinations could in fact provide valuable advancements for the state-of-the-art in \gls{ADT} and \gls{DTM}.

In summary we can conclude that our results strongly indicate and strengthens our hypothesis that combining different datasets with variable characteristics, will help a model perform better on \gls{ADT} tasks, and could make them generalize better towards \acrfull{OOD} datasets. We also note that applicability of different \gls{ADT} tasks follow a relationship, where \gls{DTM} datasets generalize better on \gls{DTD} datasets than the inverse.

\textcolor{red}{I'm unsure if I've discussed enough (or too much of certain aspects) around the results presented in \ref{DatasetResultsTable}?}