\chapter{Dataset Study}\label{Study2}

The primary objective of this study is to investigate how different datasets, and their unique characteristics, influence model performance, both within-domain and under \gls{OOD} conditions. This insight can help guide how existing datasets are best utilized or combined, and how future datasets might be intelligently constructed to improve model performance and generalization in \acrfull{ADT} and \acrfull{DTM}.

To isolate the effect of dataset composition, all models in this study use the \acrfull{CRNN} architecture, identified in the previous chapter as the most effective for \gls{ADT}.

\section{Methodology}

I train \acrshort{CRNN} models on various combinations of the four main datasets: ENST+MDB, E-GMD, Slakh, and ADTOF-YT. Dataset combinations are treated as unions, with one training epoch corresponding to a full pass over all included datasets. Each trained model is then evaluated not only on the test split(s) of the dataset(s) it was trained on, but also on all other dataset test sets, including the unseen SADTP dataset, to measure \gls{OOD} generalization.

To strike a balance between coverage and feasibility, I strategically select 10 dataset combinations. These are chosen to provide meaningful insight into how different dataset properties affect generalization, while avoiding redundant experiments with diminishing returns.

\section{Results}

\begin{table}[H]
    \centering
    \hspace*{-1.0cm}
    \resizebox{1.13\linewidth}{!}{%
    \begin{tabular}{l|ccccc}
        Training Datasets & ENST+MDB & E-GMD & Slakh & ADTOF-YT & SADTP      \\
        \hline
        ENST+MDB	& 0.81	& \cellcolor{blue!5} 0.59	& \cellcolor{blue!5} 0.53	& \cellcolor{blue!5} 0.60	& \cellcolor{blue!5} 0.42 \\
        E-GMD	& \cellcolor{blue!5} 0.55	& \textbf{0.90}	& \cellcolor{blue!5} 0.44	& \cellcolor{blue!5} 0.42	& \cellcolor{blue!5} 0.28 \\
        Slakh	& \cellcolor{blue!5} 0.80	& \cellcolor{blue!5} 0.73	& \textbf{0.90}	& \cellcolor{blue!5} 0.59	& \cellcolor{blue!5} 0.48 \\
        ADTOF-YT	& \cellcolor{blue!5} 0.84	& \cellcolor{blue!5} 0.69	& \cellcolor{blue!5} \underline{0.65}	& 0.93	& \cellcolor{blue!5} 0.60 \\
        \hline
        ENST+MDB + Slakh	& 0.84	& \cellcolor{blue!5} 0.73	& \textbf{0.90}	& \cellcolor{blue!5} \underline{0.63}	& \cellcolor{blue!5} 0.48 \\
        ENST+MDB + ADTOF-YT	& 0.86	& \cellcolor{blue!5} 0.70 	& \cellcolor{blue!5} 0.63	& 0.94	& \cellcolor{blue!5} \textbf{\underline{0.62}} \\
        Slakh + ADTOF-YT	& \cellcolor{blue!5} \underline{0.86}	& \cellcolor{blue!5} 0.72	& \textbf{0.90}	& \textbf{0.97}	& \cellcolor{blue!5} \textbf{\underline{0.62}} \\
        \hline
        ENST+MDB + Slakh + ADTOF-YT	& \textbf{0.88}	& \cellcolor{blue!5} \underline{0.74}	& 0.89	& 0.95	& \cellcolor{blue!5} 0.61 \\
        E-GMD + Slakh + ADTOF-YT	& \cellcolor{blue!5} 0.85	& \textbf{0.90} 	& 0.89	& 0.93	& \cellcolor{blue!5} 0.61 \\
        \hline
        ENST+MDB + E-GMD + Slakh + ADTOF-YT	& 0.87	& 0.89	& \textbf{0.90} 	& 0.93	& \cellcolor{blue!5} \textbf{\underline{0.62}} \\
    \end{tabular}%
    }
    \caption{Micro F1-scores for a \acrlong{CRNN} trained on various dataset combinations and evaluated across all datasets. \textbf{Bold} values represent the highest score achieved for that dataset. \underline{Underlined} values represent the highest score achieved in \acrfull{OOD} evaluation for that dataset. Cells shaded in \colorbox{blue!10}{light blue} indicate \acrshort{OOD} evaluations.}
    \label{DatasetResultsTable}
\end{table}

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{l|cccc}
        Model & ENST+MDB & E-GMD & Slakh & ADTOF-YT      \\
        \hline
        Slakh + ADTOF-YT & \cellcolor{blue!5} 0.86	& \cellcolor{blue!5} 0.72 & \textbf{0.90}	& \textbf{0.97} \\
        ENST+MDB + Slakh + ADTOF-YT & \textbf{0.88}	& 0.74 & \cellcolor{blue!5} 0.89	& 0.95 \\
        E-GMD + Slakh + ADTOF-YT & \cellcolor{blue!5} 0.85 & \textbf{0.90} & 0.89 & 0.93 \\
        \hline
        OaF-Drums~\cite{callender2020improvingperceptualqualitydrum} & \cellcolor{blue!5} 0.77/$\times$* & 0.83 & $\times$ & $\times$ \\
        \hline
        MT3 (mixture)~\cite{gardner2022mt3multitaskmultitrackmusic} & $\times$ & $\times$ & 0.76 & $\times$ \\
        \hline
        YPTF.MoE+M~\cite{chang2024yourmt3+} & 0.87/$\times$** & $\times$ & 0.85 & $\times$ \\
        \hline
        ADTOF-RGW + ADTOF-YT~\cite{signals4040042} & \cellcolor{blue!5}0.78/0.81*** & $\times$ & $\times$ & 0.85 \\
    \end{tabular}
    \caption{Micro F1-scores for selected \acrlong{CRNN} models from this dataset study, compared with the best-performing models reported in related literature. \textbf{Bold} values represent the highest score achieved for that dataset. Cells shaded in \colorbox{blue!10}{light blue} indicate \acrshort{OOD} evaluations. \newline
    (*) Callender et. evaluate only on ENST-Drums, using isolated drum stems in a \gls{DTD} setting and a different test split~\cite{callender2020improvingperceptualqualitydrum}. \newline
    (**) Chang et. al evaluate only ENST-Drums, also using a different test split~\cite{chang2024yourmt3+}. \newline
    (***) Zehren et. al report separate scores for ENST-Drums and MDB Drums using different test splits~\cite{signals4040042}.
    }
    \label{DatasetComparisonTable}
\end{table}

\section{Discussion}

The results from the dataset study, summarized in Table \ref{DatasetResultsTable} reveal important insights into how dataset composition affects model generalization. Notably, these findings demonstrate that strategically combining \gls{ADT} datasets can significantly improve both within-domain and \acrfull{OOD} generalization performance.

Firstly, it is evident that each dataset yields strong performance on test splits drawn from the same distribution as its training data, but substantially lower performance under \gls{OOD} conditions. In other words, all models exhibit a noticeable \textit{transfer gap}, a decline in performance when evaluated outside their training domain. This behaviour suggests that the datasets differ sufficiently in their characteristics to introduce meaningful domain shifts, which in turn hinder generalization.

This generalization penalty appears relatively consistent across most datasets, with the exception of models trained on E-GMD (discussed in the next paragraph). Most of the best performing \gls{OOD} results fall within a micro F1-score range of $0.6$ to $0.7$, underscoring the difficulty of cross-domain generalization in \gls{ADT}. A notable outlier is the ENST+MDB test set, where the model trained on Slakh+ADTOF-YT achieves a remarkably high \gls{OOD} F1-score of $0.86$, nearly matching its best within-domain score of $0.88$. 

I hypothesize that this unusually small transfer gap can be attributed to the exceptional annotation quality of ENST+MDB. Unlike the other datasets, which may contain alignment errors or inconsistencies, ENST+MDB consists of highly accurate and precisely aligned \gls{DTM} annotations. This likely makes it easier for models trained on other datasets to demonstrate their learned capabilities when evaluated on ENST+MDB, whereas noisier test sets might obscure such strengths. As a result, performance on ENST+MDB may reflect model competence more transparently than other datasets.

Another important observation concerns the E-GMD dataset. Unlike the others, E-GMD is a \acrfull{DTD} dataset containing only isolated drums stems, without any melodic accompaniment. Despite its large size, models solely trained on E-GMD generalize poorly to other datasets. In fact, it performs worst across all \gls{OOD} evaluations, with F1-scores as low as $0.28$ when evaluated on SADTP. This highlights a significant transfer gap between \gls{DTD} and \gls{DTM} tasks, suggesting that training on isolated drum tracks does not readily transfer to full musical mixtures. 

However, it is worth noting that E-GMD achieves the highest within-domain performance when evaluated on its own test set. This contrast reinforces the distinction between \gls{DTD} and \gls{DTM} as separate problem formulations. It also suggests that models trained on one task may not be directly applicable to the other.

Building on this observation, we can consider this reverse relationship, how well models trained on \gls{DTM} datasets perform on the \gls{DTD} task. Interestingly, the \gls{OOD} performance of models trained on \gls{DTM} datasets when evaluated on E-GMD is comparable to that their performance on other unseen \gls{DTM} datasets. This supports the hypothesis that \gls{DTM} may encompass \gls{DTD} in terms of task complexity. In other words, while \gls{DTM} poses an additional challenge due to the presence of melodic accompaniment, it may equip models with more generalizable skills, allowing them to perform reasonably well on \gls{DTD} task without explicit exposure.

I argue that this asymmetry stems from the structural relationship between the two tasks. \gls{DTD} can be seen as a strict subset of \gls{DTM}, since \gls{DTM} datasets often include passages where drums play in isolation (e.g., drum solos), where as \gls{DTD} datasets never include accompaniment. A model trained on \gls{DTM} can thus learn to transcribe drums both in isolation and in mixture, while a model trained solely on \gls{DTD} lacks exposure to the contextual variability needed for generalization to \gls{DTM}. This may explain why \gls{DTM}-trained models exhibit some degree of zero-shot generalization to \gls{DTD}, whereas the reverse does not hold.

One result that merits further discussion is the observed correlation between high performance on ADTOF-YT and high \gls{OOD} performance on SADTP. Models that perform well on the ADTOF-YT test split also tend to achieve relatively high F1-scores when evaluated on SADTP. Given the crowdsourced nature of ADTOF-YT and the fact that SADTP consists of real-time performed contemporary music tracks, there may be a degree of distributional overlap that explains this trend.

However, I hypothesize that generalization ability does not arise from direct overlap in audio content, but rather from the heterogeneity inherent in the ADTOF-YT dataset. Because it was constructed through crowdsourcing, ADTOF-YT includes recordings from a broad range of sources, audio quality, and playing styles. This diversity likely results in a more varied training distribution, one that better captures the domain complexity found in the real-world, user-generated datasets like SADTP.

By contrast, the other datasets used in this study are significantly more homogeneous. ENST+MDB consists of a small group of professional drummers recorded in similar, pristine studio conditions; E-GMD and Slakh are both fully synthesized, lacking the variability of live performances and/or recording environments. This domain uniformity likely limits their ability to generalize, especially when transitioning from synthetic or curated domains to more realistic ones.

This interpretation aligns with prior findings on the so-called synthetic-to-real transfer gap, a well documented challenge in both \gls{ADT} and other fields~\cite{zehren2024analyzingreducingsynthetictorealtransfer}. Based on these observations, I argue that dataset heterogeneity, particularly when achieved through crowdsourcing, is an invaluable asset for improving model generalization. Providing that data quality is maintained, crowdsourcing offers a practical and effective means of capturing real-world variability, and should be considered a priority in the construction of future \gls{ADT} and \gls{DTM} datasets.

An insightful observation is that the best performance of all \gls{DTM} datasets from the architecture study, sees themselves beat by models trained on a combination of datasets. That is all except for models which themselves reach the maximum micro F1-score when trained singularly on themselves, such as E-GMD and Slakh. This is an important observation and shows that model performance within-domain is often enhanced by expanding the dataset size and variation. I deem this find surprising, through expanding the size of a dataset with data from other domains, one inherently \textit{dillutes} the domain of our own. This however, does thus not appear to significantly lower performance on-distribution, and proves valuable when training models on dataset to generalize within-domain. Although it is worth to note that this is not always the case, as several models experience a drop in performance from the addition of other datasets. For example on ADTOF-YT, which has a maximum micro F1-score of $0.97$ when trained on Slakh+ADTOF-YT, which drops down to a $0.94$ when trained together with ENST+MDB. This performance drop becomes even more noticeable when trained together with E-GMD, where it drops to $0.93$. This also builds upon what we discussed in the last paragraph, it is important to understand the differences between \gls{DTD} and \gls{DTM} datasets. As we can observe in Table~\ref{DatasetResultsTable}, the best Micro F1-score for \gls{DTD} dataset comes from training data solely of \gls{DTD} data. The same holds the other way, in that the best performances on \gls{DTM} datasets happen when solely trained on \gls{DTM} data. It is from this information I argue that supplementing a \gls{DTM} dataset and model with \gls{DTD} data lowers the complexity of the data in such a way which penalizes the performance of the model. Thus, ensuring that the data trained on matches the complexity of the task is important when constructing datasets for \gls{ADT}.

Secondly, take a look at the model trained on the largest amount of data, the combination of all datasets. This model exhibits very good performance across the board, however interestingly it does not achieve the highest Micro F1-score on any, except for Slakh, where it performs as good as some of the others. This strengthens the hypothesis that expanding the amount data will heighten generalization ability for a given model, and agrees with the current consensus that training dataset size is vital to achieving an optimally generalizing model~\cite{signals4040042, 9747048}. For future works it would be interesting to analyze how valuable data augmentation would be for \gls{ADT}, and if it affects generalization positively.

Lastly, in Table \ref{DatasetComparisonTable} we compare our best performing model's with other literature and giving remarkable insight. Note that not much literature exist with models evaluated on thsee datasets. Gardner et. al's MT3~\cite{gardner2022mt3multitaskmultitrackmusic} and Chang et. al's YPTF.MOE+M~\cite{chang2024yourmt3+} are general \gls{AMT} models, predicting several different instruments in addition to drums. They also present using the Onset F1-score terminology, equivalent to our Micro F1-score (also used by Zehren et. al~\cite{signals4040042}). Zehren et. al's ADTOF-RGW + ADTOF-YT is a \gls{DTM} model, similar to that of ours. Observably, we note that our model's outperform the others' by a singificant amount on the E-GMD, Slakh, and ADTOF-YT datasets. Specifically, the F1-score of the Slakh+ADTOF-YT model is remarkably high, having an F1-score of $0.97$. Due to the splits and datasets being a bit different for ENST+MDB, a direct comparison is a bit more difficult, but one could reason that our model's performance is comparable to that of YPTF.MOE+M~\cite{chang2024yourmt3+}. All in all, these comparative performances do strengthen the fact that these models, trained on these selected datasets, with a well performing architecture, outperform other state-of-the-art models from related literature by a significant amount. By comparing with results from other literature, we can put our model's performance into perspective and strengthens the argument that our combinations can provide valuable advancements for the state-of-the-art in \gls{ADT} and \gls{DTM}.

Before we conclude it is worth to note that this study includes the same uncertainties from the previous study. Random chance could still have a significant influence over the quality of our resulting models, and it is impossible to say what the absolute reason for a model's optimal performance is. However, from the methodological design choices we've applied, the effects of these uncontrollable factors are tried minimized, and we can therefore trust that our resulting performances come as a result of our controllable choices, and not from noise, random chance, or luck.

In summary I conclude that our results strongly indicate and strengthens our hypothesis that combining different datasets with variable characteristics, and focusing on creating larger, crowdsourced datasets, and ensuring that the domain and complexity of the data matches that of the task, helps a model perform significantly better on \gls{ADT} tasks, and could make them generalize better both within-domain, but most importantly on \acrfull{OOD} datasets. We also note that applicability of different \gls{ADT} tasks follow a hierarchical relationship, where \gls{DTM} datasets generalize better on \gls{DTD} datasets than the inverse.