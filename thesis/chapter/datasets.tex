\chapter{Datasets}\label{Datasets}

\section{ENST+MDB}

The ENST-Drums dataset by Gillet and Richard~\cite{gillet2006enst} is one of the most commonly used \gls{ADT} datasets~\cite{8350302}. It features thoroughly annotated, real drum recordings performed by three drummers across various musical genres. Most tracks are drum-only, except for the \textit{minus-one} subset, which includes accompaniment. Since this thesis focuses on \gls{DTM}, I limit my use to subset.

The dataset provides separate audio files for the drum performance and the accompaniment. I additively combine these to form a single mixture track. Although multiple microphone recordings from different spatial positions are available for each performance, I exclusively use the \textit{"wet mix"} as it combines the individual channels into a polished recording that closely resembles a professionally mixed track.

ENST-Drums contains 1.02 hours of music across 64 tracks and was accessed via Zenodo~\cite{gillet_2006_7432188}.

The MDB Drums dataset by Southall et al.~\cite{southall2017mdb} is another well-known dataset used for \gls{ADT}. It is built on Bittner et al.'s MedleyDB (MDB) dataset~\cite{bittner2014medleydb}, but has been re-annotated and specifically adapted for \gls{ADT}-related transcription tasks. Like ENST-Drums, it is split into different stem tracks, including isolated drum recordings and accompaniement. However, it also provides pre-mixed \textit{full mix} tracks, which I use in this thesis.

MDB Drums include transcriptions at two levels of granularity: a \textit{class} file with 6 broad drum classes, and a \textit{subclass} file with 21 more specific drum instruments. While the 6-class transcriptions may seem more aligned with a 5-instrument setup, as used in this thesis, I found that the 21-class transcriptions offered more precise control over mapping specific instruments into the 5-instrument vocabulary. Therefore, I use the 21-instrument subclass transcriptions as annotations.

MDB Drums contains 0.35 hours of music across 23 tracks and was accessed via GitHub~\cite{southall_mdbdrums_2017}.

Both datasets distribute their audio as waveform files and their annotations as plain text files, formatted by onset time and instrument label. They are also relatively small in size and contain real, thoroughly annotated data. Due to these similarities, I combine them into a single, slightly larger dataset referred to as ENST+MDB.

In total, ENST+MDB contains 1.37 hours of music across 87 tracks, split into 896, 236, and 152 respective training, validation and test sequences.

\subsection{Splits}

Neither dataset comes with predefined train/validation/test splits, such I construct my own. From ENST-Drums, I use recordings from \textit{drummer1} and \textit{drummer2} for training. The remaining drummer, \textit{drummer3}, is split evenly between validation and test sets. Since MDB Drums does not include explicit drummers, I instead split the data by musical genre. Full details of the splits are provided in Appendix~\ref{appendix-a}.

\subsection{Mapping}

The ENST-Drums and MDB Drums datasets contain transcriptions of 20 and 21 distinct drum instruments, respectively. This does not align with the 5-instrument vocabulary used in this thesis. To utilize all the available annotations, I apply a mapping from each original annotated instrument to one of the five target classes. Table~\ref{ENSTMDBMapping} shows the mapping used for the combined ENST+MDB dataset.

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{c|ll}
        Vocabulary mapping & ENST-Drums & MDB-Drums \\
        \hline
        \acrfull{KD} & \textit{bd} & \textit{KD} \\
        \acrfull{SD} & \textit{sd}, \textit{sweep}, \textit{rs}, \textit{sticks}, \textit{cs} & \textit{SD}, \textit{SDB}, \textit{SDD}, \textit{SDF}, \textit{SDG}, \textit{SDNS}, \textit{SST} \\
        \acrfull{TT} & \textit{mt}, \textit{mtr}, \textit{lmt}, \textit{lt}, \textit{ltr}, \textit{lft} & \textit{HIT}, \textit{MHT}, \textit{HFT}, \textit{LFT} \\
        \acrfull{HH} & \textit{chh}, \textit{ohh} & \textit{CHH}, \textit{OHH}, \textit{PHH}, \textit{TMB} \\
        \acrfull{CC+RC} & \textit{cr}, \textit{spl}, \textit{ch}, \textit{rc}, \textit{c}, \textit{cb} & \textit{RDC}, \textit{RDB}, \textit{CRC}, \textit{CHC}, \textit{SPC} \\
    \end{tabular}
    \caption{Mapping from the original instrument annotations in ENST-Drums and MDB Drums to the 5-instrument vocabulary used in this thesis. Description of the original instruments are available in their respective source papers~\cite{gillet2006enst, southall2017mdb}.}
    \label{ENSTMDBMapping}
\end{table}

It should be noted that ENST-Drums includes more specific instrument annotations, where some instrument labels end with a number (e.g., \textit{"rc3"}) to indicate that multiple instances of the same instrument were present in the drum set, with the number specifying which was played. Additionally, certain snare drum events end with a hyphenated suffix (e.g., \textit{"sd-"}), indicating that the onset corresponds to a softer \textit{ghost note} hit~\cite{gillet2006enst}. Following general \gls{ADT} practice and for consistency with other datasets, I ignore these suffixes and treat such events as standard onsets for their respective instruments.

\section{E-GMD}

The Expanded Groove MIDI Dataset (E-GMD) from Callender et al.~\cite{callender2020improvingperceptualqualitydrum} is a large \gls{ADT} dataset consisting of audio recordings from human drum performances annotated in MIDI. It is an expansion of Gillick et al.'s Groove MIDI Dataset (GMD)~\cite{pmlr-v97-gillick19a}.

GMD was created through recording human performances in MIDI format through a Roland TD-11 electronic drumkit. This dataset does not contain audio recordings, only MIDI files, such that it is not applicable for \gls{ADT}. To expand this dataset to be used with \gls{ADT}, Callender et al. re-recorded the MIDI sequences on a Roland TD-17 electronic drumkit in real-time on a \gls{DAW}. These re-recordings were done over a large amount of differing soundfonts (confusingly also called \textit{drumkits} on electronic drumkits), synthesizing several differently sounding audio recordings from a single MIDI performance~\cite{pmlr-v97-gillick19a, callender2020improvingperceptualqualitydrum}.

In contrast to the other dataset's utilized in this thesis, this dataset is not created for a \gls{DTM} task, but rather a \gls{DTD} task. This is due to all recordings containing solo, drum-only performances. In addition, as this data is recorded from human performances in a semi-manual nature, there exist some errors from the recording process \textit{"that resulted in unusable tracks."}~\cite{callender2020improvingperceptualqualitydrum}. The magnitude of these errors are not stated, however other authors propose that it might be as high as 20.5\% of the tracks with varying amounts of discrepancies~\cite{holz2021automatic}.

This dataset contains 444.5 hours of audio recordings, from 1,059 unique drum performances resampled to 45,537 MIDI sequences, accessed through Zenodo~\cite{callender_2020_4300943}.

\subsection{Mapping}

The E-GMD dataset is annotated on a Roland electric drumkit and stored in MIDI files. These files store the instruments as specific pitches, with the pitch-to-instrument mapping being given by the standard \textit{General MIDI} percussion key map, as well as specific Roland default MIDI mappings~\cite{roland_drum_midi_td_17}.

This \textit{General MIDI} percussion mapping is extensive, containing MIDI pitches for vast amounts of percussive instruments. Many of these either do not appear in drumset transcriptions, or do not have a logical 5-instrument mapping. Such instruments are ignored, and are omitted from the mapping.

General MIDI is a widely used standard, and is extensively documented. The specific MIDI percussion pitch to drum instrument can be found in e.g. "MIDI: A comprehensive introduction" by Rothstein~\cite{rothstein1995midi}, and the Roland specific MIDI pitches in their documentation~\cite{roland_drum_midi_td_17}. In case there are two different instruments marked with the same MIDI pitch, the one from Roland is selected.

\textcolor{red}{Show the mapping directly from Groove MIDI dataset instead (GMD) and Cite!}

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{c|ll}
        Vocabulary mapping & MIDI Pitch \\
        \hline
        \acrfull{KD} & \textit{35}, \textit{36} \\
        \acrfull{SD} & \textit{37}, \textit{38}, \textit{39}, \textit{40} \\
        \acrfull{TT} & \textit{41}, \textit{43}, \textit{45}, \textit{47}, \textit{48}, \textit{50}, \textit{58} \\
        \acrfull{HH} & \textit{22}, \textit{26}, \textit{42}, \textit{44}, \textit{46}, \textit{54} \\
        \acrfull{CC+RC} & \textit{49}, \textit{51}, \textit{52}, \textit{53}, \textit{55}, \textit{56}, \textit{57}, \textit{59} \\
    \end{tabular}
    \caption{The mapping for each of the Roland TD-11 and General MIDI percussion pitches onto the 5-instrument vocabulary used in this thesis. MIDI pitches omitted do not have a intuitive 5-instrument drum mapping and are simply ignored.}
    \label{RolandMIDIMapping}
\end{table}

\section{Slakh}

The Synthesized Lakh 2100 (Slakh) Dataset from Manilow et al.~\cite{8937170} is a synthesized version of a subset of the Lakh Dataset from Raffel~\cite{raffel2016learning}, that subset being a 2100 randomly selected tracks from Lakh where the MIDI files contain at least a piano, bass, guitar, and drums. These MIDI files are rendered and mixed into combined audio files, stored together with their respective original MIDI performances.

As mentioned, this dataset contains more instruments than just the drumset, such that it is originally meant for a \gls{AMT} task. However, due to each track being guaranteed to contain a drum performance, converting it to an \gls{ADT} task is trivial, and is done by selectively only utilizing the MIDI files corresponding to the drumset as our labels.

The original Slakh dataset was found to have data leakage between the different splits, and it is therefore recommended for transcription tasks to use a smaller subset, Slakh2100-redux, where this issue has been solved. Therefore, this is the dataset used for this thesis.
\textcolor{red}{Needs citation, other than the github and zenodo? \\ \url{https://github.com/ethman/slakh-utils}, \url{https://zenodo.org/records/4599666}}

This dataset contains 115 hours of audio recordings over 1709 different songs, accessed through Zenodo~\cite{manilow_2019_4599666}.

\subsection{Mapping}

The Slakh dataset stores its annotations in MIDI files, similarly to E-GMD. Contrary to E-GMD it strictly uses the General MIDI percussion mapping. This mapping is very similar, only excluding the specific MIDI pitches \textit{22}, \textit{26}, and \textit{58}, seen in Table \ref{MIDIMapping}.

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{c|ll}
        Vocabulary mapping & MIDI Pitch \\
        \hline
        \acrfull{KD} & \textit{35}, \textit{36} \\
        \acrfull{SD} & \textit{37}, \textit{38}, \textit{39}, \textit{40} \\
        \acrfull{TT} & \textit{41}, \textit{43}, \textit{45}, \textit{47}, \textit{48}, \textit{50} \\
        \acrfull{HH} & \textit{42}, \textit{44}, \textit{46}, \textit{54} \\
        \acrfull{CC+RC} & \textit{49}, \textit{51}, \textit{52}, \textit{53}, \textit{55}, \textit{56}, \textit{57}, \textit{59} \\
    \end{tabular}
    \caption{The mapping for each of General MIDI percussion pitches onto the 5-instrument vocabulary used in this thesis. MIDI pitches omitted do not have an intuitive 5-instrument drum mapping and are simply ignored.}
    \label{MIDIMapping}
\end{table}

\section{ADTOF-YT}

The Automatic Drums Transcription On Fire YouTube (ADTOF-YT) dataset from Zehren et al.~\cite{signals4040042} is a large \gls{ADT} dataset containing crowdsourced data, hence the YouTube suffix. Due to the crowdsourced nature of this dataset, it is possible to utilize large amounts of non-synthetic, human data, but with the tradeoff in which we can not guarantee its quality. It is also worth to be noted ADTOF-YT seems to be biased towards metal and rock musical genres, as noticed by Zehren et al.~\cite{signals4040042}.

Contrary to the other datasets, this one is distributed in prepackaged TensorFlow datasets for each split, with datapoints as pairs of logarithmically filtered log-spectograms and sequence of instrument onset probabilities. This dataset also comes with a vocabulary of 5, and is thus the least diverse dataset in this thesis.

In their original paper, "High-Quality and Reproducible Automatic Drum Transcription From Crowdsourced Data", they state that their spectrograms are stored as log-magnitude, log-frequency spectrograms~\cite{signals4040042}. This contradicts information from where they distribute their dataset, where they state that \textit{"The data as mel-scale spectrograms."}~\cite{zehren_2023_10084511}. Through manual verification and approximation of the original waveforms, we propose that this latter information is an error, and confirms that the spectrograms are indeed stored as logarithmically filtered spectrograms, following the same format used by Vogl et. al~\cite{Vogl2017DrumTV} and what is otherwise used in this thesis.
\textcolor{red}{Is this enough justification? Should I provide more information? If so, what exactly?}

The dataset contains 245 hours of music over 2924 tracks, and was accessed through Zenodo~\cite{zehren_2023_10084511}.

\subsection{Mapping}

This dataset is distributed already on a 5-instrument vocabulary fitting the one used in this thesis. Therefore, there was no need to remap the annotations; the dataset was used "as is".

\section{SADTP}

The SADTP (Small Automatic Drum Transcription Performance) dataset is a novel dataset introduced in this thesis. It is a small dataset comprised of 16 songs with corresponding MIDI transcriptions. 

The \textit{performance} name alludes to the transcription being recorded live while listening to the songs on playback, with only minor post-processing. The transcriptions were recorded on a Roland TD-11 electric drumset, recording the MIDI perfomance to Apple's Garageband \gls{DAW}, and extracting them to separate MIDI files. This comes with a similarly to E-GMD, as this dataset also was recorded in a semi-manual nature, which opens the possibility for slight, human induced errors. The magnitude of such errors are however not known, but we speculate that it is small but not insignificant.

This dataset stands out, as it is the only one in this thesis not used for training. Its sole purpose is for cross-dataset evaluation, and to provide information on the generalization ability for models trained on data from other sources.

The dataset contains 1.08 hours of music, which can be split into 977 non-overlapping 4 second datapoints (includes zero padding certain pieces for even partitioning), accessed through GitHub~\cite{fosse_sadtp_2025}.

\subsection{Mapping}

Similarly to E-GMD, the SADTP dataset stores its annotations in MIDI files recorded from a Roland TD-11 electric drumset. Hence, it applies the same Roland MIDI mapping given in Table \ref{RolandMIDIMapping} making it suitable for a 5-instrument vocabulary.

\section{Summary}

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{l|cccc}
        Dataset & Duration (h) & Number of tracks & Melodic & Synthetic \\
        \hline
        ENST+MDB & 1.37 & 87 & $\checkmark$ & $\times$ \\
        E-GMD & 444.5 & 45,537 & $\times$ & $\checkmark$ \\
        Slakh & 115 & 1709 & $\checkmark$ & $\checkmark$ \\
        ADTOF-YT & 245 & 2915 & $\checkmark$ & $\times$ \\
        SADTP & 1.08 & 977 & $\checkmark$ & $\times$ \\
    \end{tabular}
    \caption{Comparison of each dataset's characteristics including total duration and number of tracks. Melodic datasets denote being a part of \gls{DTM}, otherwise they are \gls{DTD}. Synthetic datasets contain music which is synthesized digitally, with an often automatic generation scheme~\cite{zehren2024analyzingreducingsynthetictorealtransfer}.}
    \label{DatasetSummaryTable}
\end{table}

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{l|ccc}
        Dataset & Train & Validation & Test \\
        \hline
        ENST+MDB & 896 & 236 & 152 \\
        E-GMD & 324,266 & 49,424 & 48,571 \\
        Slakh & 80,662 & 16,643 & 9963 \\
        ADTOF-YT & 134,332 & 28,984 & 18,650 \\
        SADTP & $\times$ & $\times$ & 977 \\
    \end{tabular}
    \caption{Comparison of the sizes of each dataset's different splits. Each dataset contains sequences of size 400 (corresponding to 4 second audio clips). These only summarize the direct size of each dataset, but does not take specific dataset augmentation into account, e.g. oversampling a MIDI sequence with differently sounding synthesized instruments.}
    \label{DatasetSplitSummaryTable}
\end{table}

In Table \ref{DatasetSummaryTable} we have a comprehensive summary of the different aspect of each of the dataset, as well as presenting them in a comparable format. These dataset span a good range of different characteristics, which could prove beneficial for training generalizing \gls{ADT} models. Table \ref{DatasetSplitSummaryTable} explicitly shows us the size differences between these datasets, and will be crucial once we start comparing performances across datasets.