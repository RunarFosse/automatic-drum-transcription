\chapter{Datasets}\label{Datasets}

\section{ENST+MDB}

The ENST-Drums dataset by Gillet and Richard~\cite{gillet2006enst} has been one of the most commonly used \gls{ADT} datasets~\cite{8350302}. It features thorougly annotated drum samples by three drummers over different musical genres. Most of the tracks contain drum-only recordings, except the \textit{minus-one} subset, which is played together with a music accompaniement. As this thesis attends to \gls{DTM} tasks, we isolate our focus to this subset of tracks.

As this dataset contains separate audio files for performance and accompaniement, we additively combine them to create a singular respective mixture track. There exist many recordings from differently placed microphones of a single performance, however in this thesis, we solely selected the \textit{"wet mix"} due to it being a combined recording of all other microphone recordings, and its \textit{mix} showing resembelance of a polished performance track.

ENST-Drums contains 1.02 hours of music over 64 tracks.

Another well-known MedleyDB Drums dataset, from Southall et al.~\cite{southall2017mdb}. This dataset is built on top of Bittner et al.'s MedleyDB (MDB) dataset~\cite{bittner2014medleydb}, but re-annotated and specialized for \gls{ADT} related tasks. This dataset is, similar to ENST-Drums, also split into different stem tracks, such as isolated drum recordings and accompaniement. However, they also contain already-mixed \textit{full mix} tracks, which are the ones we use in this thesis.

Tracks in MDB-Drums come transcribed in two different granularities. First, a \textit{class} file, split into 6 larger drum classes, and a \textit{subclass} file, containing a more specific 21 drum instruments. Although the 6-class transcriptions may seem closer to a 5-instrument vocabulary, the 21 gave better control over which specific instruments coincided with which in the 5-instrument vocabulary. Therefore, these subclass transcriptions are utilized as annotations.

MDB-Drums contains 0.35 hours of music over 23 tracks.

Both of these datasets distributes their audio in waveform files, and their annotations in text files. Annotations are formatted by onset time and instrument label. They are also relatively small, and contain thorough, real, annotated data. Due to these similarities, in this thesis they are combined together into a slightly larger ENST+MDB dataset.

In total, this dataset contains 1.37 hours of music over 87 tracks, split into 896, 236 and 152 respective training, validation and test sequences.

\subsection{Splits}

These two datasets do not have predefined train/validation/test splits, such that we decided to construct our own splits. From ENST-Drums, \textit{drummer1} and \textit{drummer2} make our training split. The remaining drummer, \textit{drummer3} is split in half, each for validation and test respectively. From MDB-Drums we do not have different explicit drummers, but instead split on specific genres. The different splits are specified in Appendix \ref{appendix-a}.

\subsection{Mapping}

The ENST-Drums and MDB-Drums datasets are transcribed with 20 and 21 respective instruments. This does not match our models' vocabulary of 5. To utilize as much data as we can, we apply a mapping for each of the original instruments to one in our 5-instrument vocabulary. Table \ref{ENSTMDBMapping} shows the mapping for the ENST+MDB dataset.

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{c|ll}
        Vocabulary mapping & ENST-Drums & MDB-Drums \\
        \hline
        \acrfull{KD} & \textit{bd} & \textit{KD} \\
        \acrfull{SD} & \textit{sd}, \textit{sweep}, \textit{rs}, \textit{sticks}, \textit{cs} & \textit{SD}, \textit{SDB}, \textit{SDD}, \textit{SDF}, \textit{SDG}, \textit{SDNS}, \textit{SST} \\
        \acrfull{TT} & \textit{mt}, \textit{mtr}, \textit{lmt}, \textit{lt}, \textit{ltr}, \textit{lft} & \textit{HIT}, \textit{MHT}, \textit{HFT}, \textit{LFT} \\
        \acrfull{HH} & \textit{chh}, \textit{ohh} & \textit{CHH}, \textit{OHH}, \textit{PHH}, \textit{TMB} \\
        \acrfull{CC+RC} & \textit{cr}, \textit{spl}, \textit{ch}, \textit{rc}, \textit{c}, \textit{cb} & \textit{RDC}, \textit{RDB}, \textit{CRC}, \textit{CHC}, \textit{SPC} \\
    \end{tabular}
    \caption{The mapping for each of the original transcribed instruments in ENST-Drums and MDB-Drums, onto the 5-instrument vocabulary used in this thesis. The description of each of the original annotated instruments can be found in their respective papers.~\cite{gillet2006enst, southall2017mdb}}
    \label{ENSTMDBMapping}
\end{table}

It should be noted that ENST-Drums come with more specific annotations, with some instruments ending with a number (e.g. \textit{"rc3"}) informing that the drumset played on contains several of that instrument, with the number specifying which of these the current onset was made on. It also contains a few specific \textit{"sd-"} annotation, meaning that the respective snare drum onset is a softer \textit{ghost note} hit~\cite{gillet2006enst}. Aligning with the general \gls{ADT} practice and other datasets, these suffixes are ignored and onsets are recognized as their original instruments.

\section{E-GMD}

The Expanded Groove MIDI Dataset (E-GMD) from Callender et al.~\cite{callender2020improvingperceptualqualitydrum} is a large \gls{ADT} dataset consisting of audio recordings from human drum performances annotated in MIDI. It is an expansion of Gillick et al.'s Groove MIDI Dataset (GMD)~\cite{pmlr-v97-gillick19a}.

GMD was created through recording human performances in MIDI format through a Roland TD-11 electronic drumkit. This dataset does not contain audio recordings, only MIDI files, such that it is not applicable for \gls{ADT}. To expand this dataset to be used with \gls{ADT}, Callender et al. re-recorded the MIDI sequences on a Roland TD-17 electronic drumkit in real-time on a \gls{DAW}. These re-recordings were done over a large amount of differing soundfonts (confusingly also called \textit{drumkits} on electronic drumkits), synthesizing several differently sounding audio recordings from a single MIDI performance~\cite{pmlr-v97-gillick19a, callender2020improvingperceptualqualitydrum}.

In contrast to the other dataset's utilized in this thesis, this dataset is not created for a \gls{DTM} task, but rather a \gls{DTD} task. This is due to all recordings containing solo, drum-only performances. In addition, as this data is recorded from human performances in a semi-manual nature, there exist some errors from the recording process \textit{"that resulted in unusable tracks."}~\cite{callender2020improvingperceptualqualitydrum}. The magnitude of these errors are not stated, however other authors propose that it might be as high as 20.5\% of the tracks with varying amounts of discrepancies~\cite{holz2021automatic}.

This dataset contains 444.5 hours of audio recordings, from 1,059 unique drum performances resampled to 45,537 MIDI sequences.

\subsection{Mapping}

The E-GMD dataset is annotated on a Roland electric drumkit and stored in MIDI files. These files store the instruments as specific pitches, with the pitch-to-instrument mapping being given by the standard \textit{General MIDI} percussion key map, as well as specific Roland default MIDI mappings~\cite{roland_drum_midi_td_17}.

This \textit{General MIDI} percussion mapping is extensive, containing MIDI pitches for vast amounts of percussive instruments. Many of these either do not appear in drumset transcriptions, or do not have a logical 5-instrument mapping. Such instruments are ignored, and are omitted from the mapping.

General MIDI is a widely used standard, and is extensively documented. The specific MIDI percussion pitch to drum instrument can be found in e.g. "MIDI: A comprehensive introduction" by Rothstein~\cite{rothstein1995midi}, and the Roland specific MIDI pitches in their documentation~\cite{roland_drum_midi_td_17}. In case there are two different instruments marked with the same MIDI pitch, the one from Roland is selected.

\textcolor{red}{Show the mapping directly from Groove MIDI dataset instead (GMD) and Cite!}

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{c|ll}
        Vocabulary mapping & MIDI Pitch \\
        \hline
        \acrfull{KD} & \textit{35}, \textit{36} \\
        \acrfull{SD} & \textit{37}, \textit{38}, \textit{39}, \textit{40} \\
        \acrfull{TT} & \textit{41}, \textit{43}, \textit{45}, \textit{47}, \textit{48}, \textit{50}, \textit{58} \\
        \acrfull{HH} & \textit{22}, \textit{26}, \textit{42}, \textit{44}, \textit{46}, \textit{54} \\
        \acrfull{CC+RC} & \textit{49}, \textit{51}, \textit{52}, \textit{53}, \textit{55}, \textit{56}, \textit{57}, \textit{59} \\
    \end{tabular}
    \caption{The mapping for each of the Roland TD-11 and General MIDI percussion pitches onto the 5-instrument vocabulary used in this thesis. MIDI pitches omitted do not have a intuitive 5-instrument drum mapping and are simply ignored.}
    \label{RolandMIDIMapping}
\end{table}

\section{Slakh}

The Synthesized Lakh 2100 (Slakh) Dataset from Manilow et al.~\cite{8937170} is a synthesized version of a subset of the Lakh Dataset from Raffel~\cite{raffel2016learning}, that subset being a 2100 randomly selected tracks from Lakh where the MIDI files contain at least a piano, bass, guitar, and drums. These MIDI files are rendered and mixed into combined audio files, stored together with their respective original MIDI performances.

As mentioned, this dataset contains more instruments than just the drumset, such that it is originally meant for a \gls{AMT} task. However, due to each track being guaranteed to contain a drum performance, converting it to an \gls{ADT} task is trivial, and is done by selectively only utilizing the MIDI files corresponding to the drumset as our labels.

The original Slakh dataset was found to have data leakage between the different splits, and it is therefore recommended for transcription tasks to use a smaller subset, Slakh2100-redux, where this issue has been solved. Therefore, this is the dataset used for this thesis.
\textcolor{red}{Needs citation, other than the github and zenodo? \\ \url{https://github.com/ethman/slakh-utils}, \url{https://zenodo.org/records/4599666}}

This dataset contains 115 hours of audio recordings over 1709 different songs.

\subsection{Mapping}

The Slakh dataset stores its annotations in MIDI files, similarly to E-GMD. Contrary to E-GMD it strictly uses the General MIDI percussion mapping. This mapping is very similar, only excluding the specific MIDI pitches \textit{22}, \textit{26}, and \textit{58}, seen in Table \ref{MIDIMapping}.

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{c|ll}
        Vocabulary mapping & MIDI Pitch \\
        \hline
        \acrfull{KD} & \textit{35}, \textit{36} \\
        \acrfull{SD} & \textit{37}, \textit{38}, \textit{39}, \textit{40} \\
        \acrfull{TT} & \textit{41}, \textit{43}, \textit{45}, \textit{47}, \textit{48}, \textit{50} \\
        \acrfull{HH} & \textit{42}, \textit{44}, \textit{46}, \textit{54} \\
        \acrfull{CC+RC} & \textit{49}, \textit{51}, \textit{52}, \textit{53}, \textit{55}, \textit{56}, \textit{57}, \textit{59} \\
    \end{tabular}
    \caption{The mapping for each of General MIDI percussion pitches onto the 5-instrument vocabulary used in this thesis. MIDI pitches omitted do not have an intuitive 5-instrument drum mapping and are simply ignored.}
    \label{MIDIMapping}
\end{table}

\section{ADTOF-YT}

The Automatic Drums Transcription On Fire YouTube (ADTOF-YT) dataset from Zehren et al.~\cite{signals4040042} is a large \gls{ADT} dataset containing crowdsourced data, hence the YouTube suffix. Due to the crowdsourced nature of this dataset, it is possible to utilize large amounts of non-synthetic, human data, but with the tradeoff in which we can not guarantee its quality. It is also worth to be noted ADTOF-YT seems to be biased towards metal and rock musical genres, as noticed by Zehren et al.~\cite{signals4040042}.

Contrary to the other datasets, this one is distributed in prepackaged TensorFlow datasets for each split, with datapoints as pairs of logarithmically filtered log-spectograms and sequence of instrument onset probabilities. This dataset also comes with a vocabulary of 5, and is thus the least diverse dataset in this thesis.

In their original paper, "High-Quality and Reproducible Automatic Drum Transcription From Crowdsourced Data", they state that their spectrograms are stored as log-magnitude, log-frequency spectrograms~\cite{signals4040042}. This contradicts information from where they distribute their dataset, where they state that \textit{"The data as mel-scale spectrograms."}~\cite{zehren_2023_10084511}. Through manual verification and approximation of the original waveforms, we propose that this latter information is an error, and confirms that the spectrograms are indeed stored as logarithmically filtered spectrograms, following the same format used by Vogl et. al~\cite{Vogl2017DrumTV} and what is otherwise used in this thesis.
\textcolor{red}{Is this enough justification? Should I provide more information? If so, what exactly?}

The dataset contains 245 hours of music over 2924 tracks.

\subsection{Mapping}

This dataset is distributed already on a 5-instrument vocabulary fitting the one used in this thesis. Therefore, there was no need to remap the annotations; the dataset was used "as is".

\section{SADTP}

The SADTP (Small Automatic Drum Transcription Performance) dataset is a novel dataset introduced in this thesis. It is a small dataset comprised of 16 songs with corresponding MIDI transcriptions. 

The \textit{performance} name alludes to the transcription being recorded live while listening to the songs on playback, with only minor post-processing. The transcriptions were recorded on a Roland TD-11 electric drumset, recording the MIDI perfomance to Apple's Garageband \gls{DAW}, and extracting them to separate MIDI files. This comes with a similarly to E-GMD, as this dataset also was recorded in a semi-manual nature, which opens the possibility for slight, human induced errors. The magnitude of such errors are however not known, but we speculate that it is small but not insignificant.

This dataset stands out, as it is the only one in this thesis not used for training. Its sole purpose is for cross-dataset evaluation, and to provide information on the generalization ability for models trained on data from other sources.

The dataset contains 1.08 hours of music, which can be split into 977 non-overlapping 4 second datapoints (includes zero padding certain pieces for even partitioning).

\subsection{Mapping}

Similarly to E-GMD, the SADTP dataset stores its annotations in MIDI files recorded from a Roland TD-11 electric drumset. Hence, it applies the same Roland MIDI mapping given in Table \ref{RolandMIDIMapping} making it suitable for a 5-instrument vocabulary.

\section{Summary}

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{l|cccc}
        Dataset & Duration (h) & Number of tracks & Melodic & Synthetic \\
        \hline
        ENST+MDB & 1.37 & 87 & $\checkmark$ & $\times$ \\
        E-GMD & 444.5 & 45,537 & $\times$ & $\checkmark$ \\
        Slakh & 115 & 1709 & $\checkmark$ & $\checkmark$ \\
        ADTOF-YT & 245 & 2915 & $\checkmark$ & $\times$ \\
        SADTP & 1.08 & 977 & $\checkmark$ & $\times$ \\
    \end{tabular}
    \caption{Comparison of each dataset's characteristics including total duration and number of tracks. Melodic datasets denote being a part of \gls{DTM}, otherwise they are \gls{DTD}. Synthetic datasets contain music which is synthesized digitally, with an often automatic generation scheme~\cite{zehren2024analyzingreducingsynthetictorealtransfer}.}
    \label{DatasetSummaryTable}
\end{table}

\begin{table}[H]
    \centering
    \hspace*{-0.6cm}
    \begin{tabular}{l|ccc}
        Dataset & Train & Validation & Test \\
        \hline
        ENST+MDB & 896 & 236 & 152 \\
        E-GMD & 324,266 & 49,424 & 48,571 \\
        Slakh & 80,662 & 16,643 & 9963 \\
        ADTOF-YT & 134,332 & 28,984 & 18,650 \\
        SADTP & - & - & 977 \\
    \end{tabular}
    \caption{Comparison of the sizes of each dataset's different splits. Each dataset contains sequences of size 400 (corresponding to 4 second audio clips). These only summarize the direct size of each dataset, but does not take specific dataset augmentation into account, e.g. oversampling a MIDI sequence with differently sounding synthesized instruments.}
    \label{DatasetSplitSummaryTable}
\end{table}

In Table \ref{DatasetSummaryTable} we have a comprehensive summary of the different aspect of each of the dataset, as well as presenting them in a comparable format. These dataset span a good range of different characteristics, which could prove beneficial for training generalizing \gls{ADT} models. Table \ref{DatasetSplitSummaryTable} explicitly shows us the size differences between these datasets, and will be crucial once we start comparing performances across datasets.